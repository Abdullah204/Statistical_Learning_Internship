{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrMFtjr45g0dYotWaCPs6x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bBfU7bBsrGe-"},"source":["vectorized gradient descent\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bn6QMbflrDHX","executionInfo":{"status":"ok","timestamp":1630312355990,"user_tz":-120,"elapsed":616,"user":{"displayName":"Abdullah Ahmad","photoUrl":"","userId":"04299824777937366842"}},"outputId":"01ec0c3a-e0ec-4eb4-b31f-3def4d2c349d"},"source":["import numpy as np                                    \n","from sklearn import datasets                                                        \n","from sklearn.linear_model import LinearRegression     \n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing      \n","import statsmodels.api as sm\n","\n","boston = datasets.load_boston()          \n","X = boston.data                          \n","Y = boston.target  \n","M, N = X.shape\n","\n","# standardization :\n","std_scaler = preprocessing.StandardScaler()\n","standardized_X = std_scaler.fit_transform(X)\n","allOnes = np.ones((len(standardized_X), 1))\n","standardized_X2 = np.hstack([allOnes, standardized_X])\n","\n","#print(score(standardized_X2,Y,beta))\n","\n","\n","#cost function calculator\n","def cost(X, Y, beta):\n","  return ((Y - (X @ beta))**2).mean()\n","# predictor function\n","def predict(X, beta):\n","  return X @ beta\n","\n","# A function that finds the R^2 Statistic).\n","def score(X, Y, beta):\n","  Y_predicted = predict(X, beta)\n","  u = ((Y - Y_predicted)**2).sum()\n","  v = ((Y - Y.mean())**2).sum()\n","  return 1 - (u/v)\n","\n","def adjScore(X, Y, beta,M,N):\n","\n","  return 1-( (1-score(X,Y,beta) )*(M-1)/(M-N-1))\n","\n","def Fval(X,Y,beta,M,N):\n","  Y_predicted = predict(X, beta)\n","  u = ((Y - Y_predicted)**2).sum()\n","  v = ((Y - Y.mean())**2).sum()\n","  return ((v-u)/(N-1))/(u/(M-N))\n","\n","# the Batch Gradient Descent function.\n","def GradientDescent(X, Y, alpha, print_cost = False):\n","  M, N = X.shape\n","  X2 = X.copy()\n","  allOnes = np.ones((len(Y), 1))               \n","  X2 = np.hstack([allOnes, X2]) # Concatenating the allOnes column to X2(for the intercept value).\n","  #np.random.seed(0)\n","  beta = np.random.uniform(-10.0, 10.0, N + 1)\n","  cost_array = []\n","  i = 1\n","\n","  while(True):\n","    cost_ = cost(X2, Y, beta)\n","    if(len(cost_array)>1 and cost_array[-1] - cost_<0.00001):\n","      break\n","    cost_array.append(cost_)\n","    if print_cost:\n","      print(\"Iteration :\", i, '\\t', \"Cost : \" + '%.7f'%cost_)\n","    i+=1\n","    beta -= (alpha * (2/M) * (X2.T @ (X2@beta - Y)))\n","\n","  return beta, cost_array\n","\n","# the normal equation function\n","def normalEqn(X, y):  \n","    beta = np.linalg.inv((X.T).dot(X)).dot(X.T).dot(y) \n","    return beta # returns array of predictors  \n","\n","beta, cost_array = GradientDescent(standardized_X, Y, 0.1, True)\n","\n","print(\"R^2:\" ,score(standardized_X2,Y,beta))\n","print(\"Adjusted R^2 :\" , adjScore(standardized_X2,Y,beta,M,N))\n","print(\"F-statistic: \" , Fval(standardized_X2,Y,beta,M,N))\n","\n","beta = normalEqn(standardized_X2,Y)\n","print()\n","print(\"beta : \" , beta)\n","print(\"using normal equation: \")\n","print()\n","print(\"Cost:\" , cost(standardized_X2,Y,beta))\n","\n","print(\"R^2:\" ,score(standardized_X2,Y,beta))\n","\n","print(\"Adjusted R^2 :\" , adjScore(standardized_X2,Y,beta,M,N))\n","print(\"F-statistic: \" , Fval(standardized_X2,Y,beta,M,N))\n","\n","regr = LinearRegression()\n","regr.fit(X, Y)\n","print(\"scikit training model score: \" , regr.score(X, Y))\n","\n","\n","model = sm.OLS(Y,sm.add_constant(X))\n","results = model.fit()\n","print(results.summary())\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Iteration : 1 \t Cost : 773.2666244\n","Iteration : 2 \t Cost : 417.8209680\n","Iteration : 3 \t Cost : 267.8479085\n","Iteration : 4 \t Cost : 179.5138954\n","Iteration : 5 \t Cost : 125.0815163\n","Iteration : 6 \t Cost : 91.0641879\n","Iteration : 7 \t Cost : 69.5530978\n","Iteration : 8 \t Cost : 55.7789714\n","Iteration : 9 \t Cost : 46.8329392\n","Iteration : 10 \t Cost : 40.9248214\n","Iteration : 11 \t Cost : 36.9441590\n","Iteration : 12 \t Cost : 34.1972402\n","Iteration : 13 \t Cost : 32.2479005\n","Iteration : 14 \t Cost : 30.8202414\n","Iteration : 15 \t Cost : 29.7386973\n","Iteration : 16 \t Cost : 28.8908481\n","Iteration : 17 \t Cost : 28.2042042\n","Iteration : 18 \t Cost : 27.6316372\n","Iteration : 19 \t Cost : 27.1421964\n","Iteration : 20 \t Cost : 26.7152964\n","Iteration : 21 \t Cost : 26.3370265\n","Iteration : 22 \t Cost : 25.9977994\n","Iteration : 23 \t Cost : 25.6908484\n","Iteration : 24 \t Cost : 25.4112636\n","Iteration : 25 \t Cost : 25.1553727\n","Iteration : 26 \t Cost : 24.9203394\n","Iteration : 27 \t Cost : 24.7039042\n","Iteration : 28 \t Cost : 24.5042131\n","Iteration : 29 \t Cost : 24.3197061\n","Iteration : 30 \t Cost : 24.1490418\n","Iteration : 31 \t Cost : 23.9910471\n","Iteration : 32 \t Cost : 23.8446817\n","Iteration : 33 \t Cost : 23.7090139\n","Iteration : 34 \t Cost : 23.5832029\n","Iteration : 35 \t Cost : 23.4664851\n","Iteration : 36 \t Cost : 23.3581647\n","Iteration : 37 \t Cost : 23.2576048\n","Iteration : 38 \t Cost : 23.1642220\n","Iteration : 39 \t Cost : 23.0774800\n","Iteration : 40 \t Cost : 22.9968858\n","Iteration : 41 \t Cost : 22.9219853\n","Iteration : 42 \t Cost : 22.8523599\n","Iteration : 43 \t Cost : 22.7876237\n","Iteration : 44 \t Cost : 22.7274203\n","Iteration : 45 \t Cost : 22.6714206\n","Iteration : 46 \t Cost : 22.6193206\n","Iteration : 47 \t Cost : 22.5708393\n","Iteration : 48 \t Cost : 22.5257168\n","Iteration : 49 \t Cost : 22.4837126\n","Iteration : 50 \t Cost : 22.4446041\n","Iteration : 51 \t Cost : 22.4081854\n","Iteration : 52 \t Cost : 22.3742656\n","Iteration : 53 \t Cost : 22.3426680\n","Iteration : 54 \t Cost : 22.3132287\n","Iteration : 55 \t Cost : 22.2857960\n","Iteration : 56 \t Cost : 22.2602290\n","Iteration : 57 \t Cost : 22.2363971\n","Iteration : 58 \t Cost : 22.2141792\n","Iteration : 59 \t Cost : 22.1934628\n","Iteration : 60 \t Cost : 22.1741436\n","Iteration : 61 \t Cost : 22.1561248\n","Iteration : 62 \t Cost : 22.1393162\n","Iteration : 63 \t Cost : 22.1236345\n","Iteration : 64 \t Cost : 22.1090020\n","Iteration : 65 \t Cost : 22.0953464\n","Iteration : 66 \t Cost : 22.0826009\n","Iteration : 67 \t Cost : 22.0707029\n","Iteration : 68 \t Cost : 22.0595947\n","Iteration : 69 \t Cost : 22.0492223\n","Iteration : 70 \t Cost : 22.0395357\n","Iteration : 71 \t Cost : 22.0304880\n","Iteration : 72 \t Cost : 22.0220362\n","Iteration : 73 \t Cost : 22.0141396\n","Iteration : 74 \t Cost : 22.0067608\n","Iteration : 75 \t Cost : 21.9998649\n","Iteration : 76 \t Cost : 21.9934192\n","Iteration : 77 \t Cost : 21.9873934\n","Iteration : 78 \t Cost : 21.9817594\n","Iteration : 79 \t Cost : 21.9764909\n","Iteration : 80 \t Cost : 21.9715633\n","Iteration : 81 \t Cost : 21.9669539\n","Iteration : 82 \t Cost : 21.9626414\n","Iteration : 83 \t Cost : 21.9586060\n","Iteration : 84 \t Cost : 21.9548293\n","Iteration : 85 \t Cost : 21.9512941\n","Iteration : 86 \t Cost : 21.9479844\n","Iteration : 87 \t Cost : 21.9448851\n","Iteration : 88 \t Cost : 21.9419825\n","Iteration : 89 \t Cost : 21.9392634\n","Iteration : 90 \t Cost : 21.9367159\n","Iteration : 91 \t Cost : 21.9343285\n","Iteration : 92 \t Cost : 21.9320908\n","Iteration : 93 \t Cost : 21.9299930\n","Iteration : 94 \t Cost : 21.9280259\n","Iteration : 95 \t Cost : 21.9261809\n","Iteration : 96 \t Cost : 21.9244501\n","Iteration : 97 \t Cost : 21.9228260\n","Iteration : 98 \t Cost : 21.9213017\n","Iteration : 99 \t Cost : 21.9198707\n","Iteration : 100 \t Cost : 21.9185269\n","Iteration : 101 \t Cost : 21.9172648\n","Iteration : 102 \t Cost : 21.9160790\n","Iteration : 103 \t Cost : 21.9149647\n","Iteration : 104 \t Cost : 21.9139172\n","Iteration : 105 \t Cost : 21.9129322\n","Iteration : 106 \t Cost : 21.9120058\n","Iteration : 107 \t Cost : 21.9111342\n","Iteration : 108 \t Cost : 21.9103138\n","Iteration : 109 \t Cost : 21.9095416\n","Iteration : 110 \t Cost : 21.9088143\n","Iteration : 111 \t Cost : 21.9081293\n","Iteration : 112 \t Cost : 21.9074837\n","Iteration : 113 \t Cost : 21.9068751\n","Iteration : 114 \t Cost : 21.9063012\n","Iteration : 115 \t Cost : 21.9057598\n","Iteration : 116 \t Cost : 21.9052489\n","Iteration : 117 \t Cost : 21.9047666\n","Iteration : 118 \t Cost : 21.9043112\n","Iteration : 119 \t Cost : 21.9038809\n","Iteration : 120 \t Cost : 21.9034742\n","Iteration : 121 \t Cost : 21.9030897\n","Iteration : 122 \t Cost : 21.9027260\n","Iteration : 123 \t Cost : 21.9023818\n","Iteration : 124 \t Cost : 21.9020560\n","Iteration : 125 \t Cost : 21.9017475\n","Iteration : 126 \t Cost : 21.9014551\n","Iteration : 127 \t Cost : 21.9011780\n","Iteration : 128 \t Cost : 21.9009153\n","Iteration : 129 \t Cost : 21.9006660\n","Iteration : 130 \t Cost : 21.9004293\n","Iteration : 131 \t Cost : 21.9002046\n","Iteration : 132 \t Cost : 21.8999912\n","Iteration : 133 \t Cost : 21.8997883\n","Iteration : 134 \t Cost : 21.8995954\n","Iteration : 135 \t Cost : 21.8994119\n","Iteration : 136 \t Cost : 21.8992372\n","Iteration : 137 \t Cost : 21.8990709\n","Iteration : 138 \t Cost : 21.8989124\n","Iteration : 139 \t Cost : 21.8987614\n","Iteration : 140 \t Cost : 21.8986174\n","Iteration : 141 \t Cost : 21.8984800\n","Iteration : 142 \t Cost : 21.8983488\n","Iteration : 143 \t Cost : 21.8982236\n","Iteration : 144 \t Cost : 21.8981040\n","Iteration : 145 \t Cost : 21.8979896\n","Iteration : 146 \t Cost : 21.8978803\n","Iteration : 147 \t Cost : 21.8977757\n","Iteration : 148 \t Cost : 21.8976755\n","Iteration : 149 \t Cost : 21.8975797\n","Iteration : 150 \t Cost : 21.8974878\n","Iteration : 151 \t Cost : 21.8973998\n","Iteration : 152 \t Cost : 21.8973153\n","Iteration : 153 \t Cost : 21.8972344\n","Iteration : 154 \t Cost : 21.8971566\n","Iteration : 155 \t Cost : 21.8970820\n","Iteration : 156 \t Cost : 21.8970103\n","Iteration : 157 \t Cost : 21.8969414\n","Iteration : 158 \t Cost : 21.8968752\n","Iteration : 159 \t Cost : 21.8968115\n","Iteration : 160 \t Cost : 21.8967503\n","Iteration : 161 \t Cost : 21.8966913\n","Iteration : 162 \t Cost : 21.8966345\n","Iteration : 163 \t Cost : 21.8965798\n","Iteration : 164 \t Cost : 21.8965271\n","Iteration : 165 \t Cost : 21.8964763\n","Iteration : 166 \t Cost : 21.8964273\n","Iteration : 167 \t Cost : 21.8963801\n","Iteration : 168 \t Cost : 21.8963345\n","Iteration : 169 \t Cost : 21.8962905\n","Iteration : 170 \t Cost : 21.8962480\n","Iteration : 171 \t Cost : 21.8962069\n","Iteration : 172 \t Cost : 21.8961673\n","Iteration : 173 \t Cost : 21.8961289\n","Iteration : 174 \t Cost : 21.8960919\n","Iteration : 175 \t Cost : 21.8960560\n","Iteration : 176 \t Cost : 21.8960214\n","Iteration : 177 \t Cost : 21.8959878\n","Iteration : 178 \t Cost : 21.8959554\n","Iteration : 179 \t Cost : 21.8959240\n","Iteration : 180 \t Cost : 21.8958935\n","Iteration : 181 \t Cost : 21.8958641\n","Iteration : 182 \t Cost : 21.8958355\n","Iteration : 183 \t Cost : 21.8958079\n","Iteration : 184 \t Cost : 21.8957810\n","Iteration : 185 \t Cost : 21.8957551\n","Iteration : 186 \t Cost : 21.8957299\n","Iteration : 187 \t Cost : 21.8957054\n","Iteration : 188 \t Cost : 21.8956817\n","Iteration : 189 \t Cost : 21.8956587\n","Iteration : 190 \t Cost : 21.8956364\n","Iteration : 191 \t Cost : 21.8956147\n","Iteration : 192 \t Cost : 21.8955937\n","Iteration : 193 \t Cost : 21.8955733\n","Iteration : 194 \t Cost : 21.8955535\n","Iteration : 195 \t Cost : 21.8955342\n","Iteration : 196 \t Cost : 21.8955156\n","Iteration : 197 \t Cost : 21.8954974\n","Iteration : 198 \t Cost : 21.8954798\n","Iteration : 199 \t Cost : 21.8954626\n","Iteration : 200 \t Cost : 21.8954460\n","Iteration : 201 \t Cost : 21.8954298\n","Iteration : 202 \t Cost : 21.8954140\n","Iteration : 203 \t Cost : 21.8953988\n","Iteration : 204 \t Cost : 21.8953839\n","Iteration : 205 \t Cost : 21.8953694\n","Iteration : 206 \t Cost : 21.8953554\n","Iteration : 207 \t Cost : 21.8953417\n","Iteration : 208 \t Cost : 21.8953284\n","Iteration : 209 \t Cost : 21.8953155\n","Iteration : 210 \t Cost : 21.8953029\n","Iteration : 211 \t Cost : 21.8952906\n","Iteration : 212 \t Cost : 21.8952787\n","Iteration : 213 \t Cost : 21.8952671\n","Iteration : 214 \t Cost : 21.8952558\n","Iteration : 215 \t Cost : 21.8952449\n","Iteration : 216 \t Cost : 21.8952342\n","Iteration : 217 \t Cost : 21.8952238\n","Iteration : 218 \t Cost : 21.8952137\n","R^2: 0.7406382498372088\n","Adjusted R^2 : 0.7337851954629887\n","F-statistic:  117.31833270858274\n","\n","beta :  [ 2.25328063e+01 -9.28146064e-01  1.08156863e+00  1.40899997e-01\n","  6.81739725e-01 -2.05671827e+00  2.67423017e+00  1.94660717e-02\n"," -3.10404426e+00  2.66221764e+00 -2.07678168e+00 -2.06060666e+00\n","  8.49268418e-01 -3.74362713e+00]\n","using normal equation: \n","\n","Cost: 21.894831181729206\n","R^2: 0.7406426641094094\n","Adjusted R^2 : 0.7337897263724629\n","F-statistic:  117.32102871125619\n","scikit training model score:  0.7406426641094095\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.741\n","Model:                            OLS   Adj. R-squared:                  0.734\n","Method:                 Least Squares   F-statistic:                     108.1\n","Date:                Mon, 30 Aug 2021   Prob (F-statistic):          6.72e-135\n","Time:                        08:32:35   Log-Likelihood:                -1498.8\n","No. Observations:                 506   AIC:                             3026.\n","Df Residuals:                     492   BIC:                             3085.\n","Df Model:                          13                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         36.4595      5.103      7.144      0.000      26.432      46.487\n","x1            -0.1080      0.033     -3.287      0.001      -0.173      -0.043\n","x2             0.0464      0.014      3.382      0.001       0.019       0.073\n","x3             0.0206      0.061      0.334      0.738      -0.100       0.141\n","x4             2.6867      0.862      3.118      0.002       0.994       4.380\n","x5           -17.7666      3.820     -4.651      0.000     -25.272     -10.262\n","x6             3.8099      0.418      9.116      0.000       2.989       4.631\n","x7             0.0007      0.013      0.052      0.958      -0.025       0.027\n","x8            -1.4756      0.199     -7.398      0.000      -1.867      -1.084\n","x9             0.3060      0.066      4.613      0.000       0.176       0.436\n","x10           -0.0123      0.004     -3.280      0.001      -0.020      -0.005\n","x11           -0.9527      0.131     -7.283      0.000      -1.210      -0.696\n","x12            0.0093      0.003      3.467      0.001       0.004       0.015\n","x13           -0.5248      0.051    -10.347      0.000      -0.624      -0.425\n","==============================================================================\n","Omnibus:                      178.041   Durbin-Watson:                   1.078\n","Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n","Skew:                           1.521   Prob(JB):                    8.84e-171\n","Kurtosis:                       8.281   Cond. No.                     1.51e+04\n","==============================================================================\n","\n","Warnings:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.51e+04. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"],"name":"stdout"}]}]}